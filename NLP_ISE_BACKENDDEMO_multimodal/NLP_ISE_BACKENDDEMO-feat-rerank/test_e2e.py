"""
ç«¯åˆ°ç«¯æµ‹è¯•è„šæœ¬

æµ‹è¯•å®Œæ•´çš„ Agent å·¥ä½œæµï¼š
1. æ„å›¾è¯†åˆ«/è·¯ç”±å†³ç­–
2. ç­–ç•¥æ‰§è¡Œï¼ˆlocal/web/hybridï¼‰
3. è¯æ®èšåˆ
4. ç­”æ¡ˆç”Ÿæˆ
5. å“åº”æ„å»º

ä½¿ç”¨æ–¹æ³•ï¼š
1. ç¡®ä¿æœåŠ¡å·²å¯åŠ¨ï¼šuvicorn backend.app:app --reload
2. è¿è¡Œæ­¤è„šæœ¬ï¼špython test_e2e.py
"""

import json
import requests
import time
from typing import Dict, List, Any
from datetime import datetime

# API åŸºç¡€URL
BASE_URL = "http://127.0.0.1:8000"

# æµ‹è¯•ç”¨ä¾‹ï¼šè¦†ç›–ä¸åŒçš„è·¯ç”±ç­–ç•¥å’Œåœºæ™¯
TEST_CASES = [
    {
        "name": "æœ¬åœ°çŸ¥è¯†åº“é—®é¢˜ - Localç­–ç•¥",
        "question": "Who is Dr. Elara Vance?",
        "expected_policy": "local",
        "expected_fields": ["answer", "sources", "routing", "latency_ms", "confidence"],
    },
    {
        "name": "æœ¬åœ°çŸ¥è¯†åº“é—®é¢˜ - Sereleia",
        "question": "Tell me about Sereleia",
        "expected_policy": "local",
        "expected_fields": ["answer", "sources", "routing", "latency_ms", "confidence"],
    },
    {
        "name": "å®æ—¶é—®é¢˜ - Webç­–ç•¥",
        "question": "What's the weather today?",
        "expected_policy": "web",
        "expected_fields": ["answer", "sources", "routing", "latency_ms", "confidence"],
        "skip_if_no_tavily": True,  # å¦‚æœæ²¡æœ‰ Tavily APIï¼Œè·³è¿‡æ­¤æµ‹è¯•
    },
    {
        "name": "æ··åˆé—®é¢˜ - Hybridç­–ç•¥",
        "question": "Explain the Vance Protocol and give the latest real-world impact",
        "expected_policy": "hybrid",
        "expected_fields": ["answer", "sources", "routing", "latency_ms", "confidence"],
        "skip_if_no_tavily": True,
    },
    {
        "name": "æ¨¡ç³Šé—®é¢˜ - LLMåˆ¤æ–­",
        "question": "What is machine learning?",
        "expected_policy": None,  # ç”± LLM åˆ¤æ–­ï¼Œä¸å›ºå®š
        "expected_fields": ["answer", "sources", "routing", "latency_ms", "confidence"],
        "skip_if_no_tavily": True,
    },
]

# å¤šæ¨¡æ€æµ‹è¯•ç”¨ä¾‹ï¼šå›¾åƒ+æ–‡æœ¬
MULTIMODAL_TEST_CASES = [
    {
        "name": "å›¾åƒå†…å®¹æè¿° - åŸºç¡€åœºæ™¯",
        "image_filename": "hkust.png",
        "question": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬åœºæ™¯ã€ç‰©ä½“å’Œæ°›å›´",
        "expected_fields": ["answer", "image_path", "query", "latency_ms", "confidence"],
        "min_answer_length": 50,
    },
    {
        "name": "å›¾åƒå¯¹è±¡è¯†åˆ« - å¤šç‰©ä½“æ£€æµ‹",
        "image_filename": "snack.png",
        "question": "åˆ—å‡ºå›¾ç‰‡ä¸­æ‰€æœ‰å¯è§çš„ç‰©ä½“",
        "expected_fields": ["answer", "image_path", "query", "latency_ms", "confidence"],
        "min_answer_length": 30,
    },
    {
        "name": "å›¾åƒæ–‡å­—æå– - OCRèƒ½åŠ›",
        "image_filename": "error_info.png",
        "question": "è¯·æå–å¹¶æ•´ç†å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹",
        "expected_fields": ["answer", "image_path", "query", "latency_ms", "confidence"],
        "min_answer_length": 10,
    },
]

# æµ‹è¯•å›¾åƒç›®å½•
TEST_IMAGES_DIR = "test_images"

class Colors:
    """ç»ˆç«¯é¢œè‰²"""
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    RESET = '\033[0m'
    BOLD = '\033[1m'


def print_header(text: str):
    """æ‰“å°æ ‡é¢˜"""
    print(f"\n{Colors.BOLD}{Colors.BLUE}{'='*80}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.BLUE}{text.center(80)}{Colors.RESET}")
    print(f"{Colors.BOLD}{Colors.BLUE}{'='*80}{Colors.RESET}\n")


def print_success(text: str):
    """æ‰“å°æˆåŠŸä¿¡æ¯"""
    print(f"{Colors.GREEN}âœ… {text}{Colors.RESET}")


def print_error(text: str):
    """æ‰“å°é”™è¯¯ä¿¡æ¯"""
    print(f"{Colors.RED}âŒ {text}{Colors.RESET}")


def print_warning(text: str):
    """æ‰“å°è­¦å‘Šä¿¡æ¯"""
    print(f"{Colors.YELLOW}âš ï¸  {text}{Colors.RESET}")


def print_info(text: str):
    """æ‰“å°ä¿¡æ¯"""
    print(f"{Colors.BLUE}â„¹ï¸  {text}{Colors.RESET}")


def test_health_check() -> bool:
    """æ£€æŸ¥æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ"""
    try:
        response = requests.get(f"{BASE_URL}/healthz", timeout=5)
        return response.status_code == 200
    except requests.exceptions.RequestException:
        return False


def test_full_workflow(question: str) -> Dict[str, Any]:
    """æµ‹è¯•å®Œæ•´å·¥ä½œæµ"""
    url = f"{BASE_URL}/api/agent/answer"
    payload = {"q": question}
    
    try:
        start_time = time.time()
        response = requests.post(url, json=payload, timeout=60)  # å¢åŠ è¶…æ—¶æ—¶é—´
        elapsed_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        response.raise_for_status()
        result = response.json()
        result["_test_elapsed_ms"] = elapsed_time
        return result
    except requests.exceptions.Timeout:
        return {"error": "è¯·æ±‚è¶…æ—¶ï¼ˆ>60ç§’ï¼‰"}
    except requests.exceptions.RequestException as e:
        return {"error": str(e)}

def test_multimodal_workflow(image_path: str, question: str) -> Dict[str, Any]:
    """æµ‹è¯•å¤šæ¨¡æ€ï¼ˆå›¾åƒ+æ–‡æœ¬ï¼‰å·¥ä½œæµ"""
    url = f"{BASE_URL}/api/agent/multimodal"
    payload = {
        "q": question,
        "image_path": image_path
    }
    
    try:
        start_time = time.time()
        response = requests.post(url, json=payload, timeout=120)  # è§†è§‰æ¨¡å‹éœ€è¦æ›´é•¿æ—¶é—´
        elapsed_time = (time.time() - start_time) * 1000
        
        response.raise_for_status()
        result = response.json()
        result["_test_elapsed_ms"] = elapsed_time
        return result
    except requests.exceptions.Timeout:
        return {"error": "è¯·æ±‚è¶…æ—¶ï¼ˆ>120ç§’ï¼‰"}
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 404:
            return {"error": "å¤šæ¨¡æ€æ¥å£ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æ˜¯å¦å·²å®ç° /api/agent/multimodal ç«¯ç‚¹"}
        return {"error": f"HTTPé”™è¯¯ {e.response.status_code}: {e.response.text}"}
    except requests.exceptions.RequestException as e:
        return {"error": str(e)}


def validate_multimodal_response(response: Dict, expected_fields: List[str], 
                                  min_answer_length: int = 0) -> tuple[bool, List[str]]:
    """éªŒè¯å¤šæ¨¡æ€å“åº”ç»“æ„"""
    errors = []
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    for field in expected_fields:
        if field not in response:
            errors.append(f"ç¼ºå°‘å­—æ®µ: {field}")
    
    # éªŒè¯ç­”æ¡ˆé•¿åº¦
    if "answer" in response:
        answer = response["answer"]
        if not isinstance(answer, str):
            errors.append("answer å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
        elif len(answer.strip()) < min_answer_length:
            errors.append(f"ç­”æ¡ˆè¿‡çŸ­: æœŸæœ›è‡³å°‘ {min_answer_length} å­—ç¬¦ï¼Œå®é™… {len(answer.strip())} å­—ç¬¦")
    
    # éªŒè¯ç½®ä¿¡åº¦
    if "confidence" in response:
        confidence = response["confidence"]
        if not isinstance(confidence, (int, float)):
            errors.append("confidence å¿…é¡»æ˜¯æ•°å­—")
        elif not (0.0 <= confidence <= 1.0):
            errors.append(f"confidence è¶…å‡ºèŒƒå›´ [0.0, 1.0]: {confidence}")
    
    # éªŒè¯å»¶è¿Ÿ
    if "latency_ms" in response:
        latency = response["latency_ms"]
        if not isinstance(latency, (int, float)) or latency < 0:
            errors.append(f"latency_ms æ— æ•ˆ: {latency}")
    
    return len(errors) == 0, errors


def print_multimodal_response_summary(response: Dict, test_case: Dict):
    """æ‰“å°å¤šæ¨¡æ€å“åº”æ‘˜è¦"""
    print(f"\n{Colors.BOLD}ğŸ“‹ å¤šæ¨¡æ€å“åº”æ‘˜è¦{Colors.RESET}")
    print(f"{'-'*80}")
    
    # å›¾åƒä¿¡æ¯
    if "image_path" in response:
        import os
        image_name = os.path.basename(response["image_path"])
        print(f"{Colors.BOLD}ğŸ–¼ï¸  å›¾åƒ:{Colors.RESET} {image_name}")
    
    # é—®é¢˜
    if "query" in response:
        print(f"{Colors.BOLD}â“ é—®é¢˜:{Colors.RESET} {response['query']}")
    
    # å»¶è¿Ÿ
    if "latency_ms" in response:
        latency = response["latency_ms"]
        latency_color = Colors.GREEN if latency < 5000 else Colors.YELLOW if latency < 10000 else Colors.RED
        print(f"\n{Colors.BOLD}â±ï¸  å¤„ç†æ—¶é—´:{Colors.RESET} {latency_color}{latency} ms{Colors.RESET}")
        if "_test_elapsed_ms" in response:
            print(f"  å®é™…è€—æ—¶: {response['_test_elapsed_ms']:.2f} ms")
    
    # ç½®ä¿¡åº¦
    if "confidence" in response:
        confidence = response["confidence"]
        conf_color = Colors.GREEN if confidence >= 0.7 else Colors.YELLOW if confidence >= 0.4 else Colors.RED
        print(f"\n{Colors.BOLD}ğŸ“Š ç½®ä¿¡åº¦:{Colors.RESET} {conf_color}{confidence:.2f}{Colors.RESET}")
    
    # ç­”æ¡ˆé¢„è§ˆ
    if "answer" in response:
        answer = response["answer"]
        preview = answer[:300] + "..." if len(answer) > 300 else answer
        print(f"\n{Colors.BOLD}ğŸ’¬ ç­”æ¡ˆ ({len(answer)} å­—ç¬¦):{Colors.RESET}")
        print(f"  {preview}")


def run_multimodal_test_case(test_case: Dict) -> Dict[str, Any]:
    """è¿è¡Œå•ä¸ªå¤šæ¨¡æ€æµ‹è¯•ç”¨ä¾‹"""
    import os
    
    print_header(f"å¤šæ¨¡æ€æµ‹è¯•: {test_case['name']}")
    
    # æ„å»ºå›¾åƒè·¯å¾„
    image_path = os.path.join(TEST_IMAGES_DIR, test_case["image_filename"])
    abs_image_path = os.path.abspath(image_path)
    
    # æ£€æŸ¥å›¾åƒæ˜¯å¦å­˜åœ¨
    if not os.path.exists(abs_image_path):
        print_error(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {abs_image_path}")
        return {"passed": False, "error": f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {test_case['image_filename']}"}
    
    print(f"{Colors.BOLD}å›¾åƒ:{Colors.RESET} {test_case['image_filename']}")
    print(f"{Colors.BOLD}é—®é¢˜:{Colors.RESET} {test_case['question']}")
    
    # æ‰§è¡Œæµ‹è¯•
    print_info("å¤„ç†å¤šæ¨¡æ€æŸ¥è¯¢...")
    response = test_multimodal_workflow(abs_image_path, test_case["question"])
    
    # æ£€æŸ¥é”™è¯¯
    if "error" in response:
        print_error(f"è¯·æ±‚å¤±è´¥: {response['error']}")
        return {"passed": False, "error": response["error"]}
    
    # éªŒè¯å“åº”
    is_valid, errors = validate_multimodal_response(
        response,
        test_case["expected_fields"],
        test_case.get("min_answer_length", 0)
    )
    
    if not is_valid:
        print_error("å“åº”éªŒè¯å¤±è´¥:")
        for error in errors:
            print_error(f"  - {error}")
        return {"passed": False, "errors": errors, "response": response}
    
    print_success("å“åº”éªŒè¯é€šè¿‡")
    
    # æ‰“å°æ‘˜è¦
    print_multimodal_response_summary(response, test_case)
    
    # æ£€æŸ¥ç­”æ¡ˆè´¨é‡
    if "answer" in response:
        answer = response["answer"]
        if len(answer.strip()) == 0:
            print_warning("ç­”æ¡ˆä¸ºç©º")
        elif any(keyword in answer for keyword in ["æ— æ³•", "é”™è¯¯", "æŠ±æ­‰", "æš‚æ—¶"]):
            print_warning("ç­”æ¡ˆå¯èƒ½åŒ…å«é”™è¯¯ä¿¡æ¯")
        else:
            print_success(f"ç­”æ¡ˆç”ŸæˆæˆåŠŸï¼ˆ{len(answer)} å­—ç¬¦ï¼‰")
    
    return {"passed": True, "response": response}

def validate_response_structure(response: Dict, expected_fields: List[str]) -> tuple[bool, List[str]]:
    """éªŒè¯å“åº”ç»“æ„"""
    errors = []
    
    for field in expected_fields:
        if field not in response:
            errors.append(f"ç¼ºå°‘å­—æ®µ: {field}")
    
    # éªŒè¯ routing ç»“æ„
    if "routing" in response:
        routing = response["routing"]
        if "policy" not in routing:
            errors.append("routing ç¼ºå°‘ policy å­—æ®µ")
        if "rationale" not in routing:
            errors.append("routing ç¼ºå°‘ rationale å­—æ®µ")
        
        # éªŒè¯ policy å€¼
        if "policy" in routing:
            policy = routing["policy"]
            if policy not in ["local", "web", "hybrid"]:
                errors.append(f"æ— æ•ˆçš„ policy å€¼: {policy}")
    
    # éªŒè¯ latency_ms ç»“æ„
    if "latency_ms" in response:
        latency = response["latency_ms"]
        required_latency_fields = ["retrieve", "rerank", "generate", "total"]
        for field in required_latency_fields:
            if field not in latency:
                errors.append(f"latency_ms ç¼ºå°‘å­—æ®µ: {field}")
    
    # éªŒè¯ sources ç»“æ„
    if "sources" in response:
        sources = response["sources"]
        if not isinstance(sources, list):
            errors.append("sources å¿…é¡»æ˜¯åˆ—è¡¨")
        else:
            for i, source in enumerate(sources):
                if "type" not in source:
                    errors.append(f"sources[{i}] ç¼ºå°‘ type å­—æ®µ")
                elif source["type"] not in ["local", "web"]:
                    errors.append(f"sources[{i}] æ— æ•ˆçš„ type å€¼: {source['type']}")
    
    # éªŒè¯ confidence èŒƒå›´
    if "confidence" in response:
        confidence = response["confidence"]
        if not isinstance(confidence, (int, float)):
            errors.append("confidence å¿…é¡»æ˜¯æ•°å­—")
        elif not (0.0 <= confidence <= 1.0):
            errors.append(f"confidence è¶…å‡ºèŒƒå›´ [0.0, 1.0]: {confidence}")
    
    return len(errors) == 0, errors


def print_response_summary(response: Dict, test_case: Dict):
    """æ‰“å°å“åº”æ‘˜è¦"""
    print(f"\n{Colors.BOLD}ğŸ“‹ å“åº”æ‘˜è¦{Colors.RESET}")
    print(f"{'-'*80}")
    
    # è·¯ç”±ä¿¡æ¯
    if "routing" in response:
        routing = response["routing"]
        policy = routing.get("policy", "unknown")
        rationale = routing.get("rationale", "æ— ç†ç”±")
        policy_color = Colors.GREEN if policy == test_case.get("expected_policy") else Colors.YELLOW
        print(f"{Colors.BOLD}è·¯ç”±ç­–ç•¥:{Colors.RESET} {policy_color}{policy}{Colors.RESET}")
        if test_case.get("expected_policy"):
            expected = test_case["expected_policy"]
            status = "âœ…" if policy == expected else "âš ï¸"
            print(f"  æœŸæœ›: {expected} {status}")
        print(f"{Colors.BOLD}å†³ç­–ç†ç”±:{Colors.RESET} {rationale}")
    
    # å»¶è¿Ÿä¿¡æ¯
    if "latency_ms" in response:
        latency = response["latency_ms"]
        print(f"\n{Colors.BOLD}â±ï¸  å»¶è¿Ÿç»Ÿè®¡:{Colors.RESET}")
        print(f"  æ£€ç´¢: {latency.get('retrieve', 0)} ms")
        print(f"  é‡æ’: {latency.get('rerank', 0)} ms")
        print(f"  ç”Ÿæˆ: {latency.get('generate', 0)} ms")
        print(f"  æ€»è®¡: {Colors.BOLD}{latency.get('total', 0)} ms{Colors.RESET}")
        if "_test_elapsed_ms" in response:
            test_time = response["_test_elapsed_ms"]
            diff = abs(test_time - latency.get("total", 0))
            print(f"  å®é™…: {test_time:.2f} ms (å·®å¼‚: {diff:.2f} ms)")
    
    # ç½®ä¿¡åº¦
    if "confidence" in response:
        confidence = response["confidence"]
        conf_color = Colors.GREEN if confidence >= 0.7 else Colors.YELLOW if confidence >= 0.4 else Colors.RED
        print(f"\n{Colors.BOLD}ğŸ“Š ç½®ä¿¡åº¦:{Colors.RESET} {conf_color}{confidence:.2f}{Colors.RESET}")
    
    # æ¥æºç»Ÿè®¡
    if "sources" in response:
        sources = response["sources"]
        local_sources = [s for s in sources if s.get("type") == "local"]
        web_sources = [s for s in sources if s.get("type") == "web"]
        print(f"\n{Colors.BOLD}ğŸ“š æ¥æºç»Ÿè®¡:{Colors.RESET}")
        print(f"  æœ¬åœ°æ¥æº: {len(local_sources)} ä¸ª")
        print(f"  ç½‘ç»œæ¥æº: {len(web_sources)} ä¸ª")
        print(f"  æ€»è®¡: {len(sources)} ä¸ª")
    
    # ç­”æ¡ˆé¢„è§ˆ
    if "answer" in response:
        answer = response["answer"]
        preview = answer[:200] + "..." if len(answer) > 200 else answer
        print(f"\n{Colors.BOLD}ğŸ’¬ ç­”æ¡ˆé¢„è§ˆ:{Colors.RESET}")
        print(f"  {preview}")


def run_test_case(test_case: Dict, skip_if_no_tavily: bool = False) -> Dict[str, Any]:
    """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
    print_header(f"æµ‹è¯•: {test_case['name']}")
    
    print(f"{Colors.BOLD}é—®é¢˜:{Colors.RESET} {test_case['question']}")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡
    if skip_if_no_tavily:
        print_info("æ­¤æµ‹è¯•éœ€è¦ Tavily APIï¼Œå¦‚æœæœªé…ç½®å°†å¯èƒ½å¤±è´¥")
    
    # æ‰§è¡Œæµ‹è¯•
    print_info("æ‰§è¡Œå®Œæ•´å·¥ä½œæµ...")
    response = test_full_workflow(test_case["question"])
    
    # æ£€æŸ¥é”™è¯¯
    if "error" in response:
        print_error(f"è¯·æ±‚å¤±è´¥: {response['error']}")
        return {"passed": False, "error": response["error"]}
    
    # éªŒè¯å“åº”ç»“æ„
    is_valid, errors = validate_response_structure(response, test_case["expected_fields"])
    
    if not is_valid:
        print_error("å“åº”ç»“æ„éªŒè¯å¤±è´¥:")
        for error in errors:
            print_error(f"  - {error}")
        return {"passed": False, "errors": errors, "response": response}
    
    print_success("å“åº”ç»“æ„éªŒè¯é€šè¿‡")
    
    # éªŒè¯è·¯ç”±ç­–ç•¥ï¼ˆå¦‚æœæŒ‡å®šäº†æœŸæœ›å€¼ï¼‰
    if test_case.get("expected_policy"):
        actual_policy = response.get("routing", {}).get("policy")
        expected_policy = test_case["expected_policy"]
        if actual_policy != expected_policy:
            print_warning(f"è·¯ç”±ç­–ç•¥ä¸åŒ¹é…: æœŸæœ› {expected_policy}, å®é™… {actual_policy}")
    
    # æ‰“å°å“åº”æ‘˜è¦
    print_response_summary(response, test_case)
    
    # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦ä¸ºç©º
    if "answer" in response:
        answer = response["answer"]
        if not answer or len(answer.strip()) == 0:
            print_warning("ç­”æ¡ˆä¸ºç©º")
        elif any(keyword in answer for keyword in ["æ— æ³•", "é”™è¯¯", "æŠ±æ­‰", "æš‚æ—¶"]):
            print_warning("ç­”æ¡ˆå¯èƒ½åŒ…å«é”™è¯¯æˆ–é™çº§ä¿¡æ¯")
        else:
            print_success(f"ç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦")
    
    return {"passed": True, "response": response}


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print_header("ç«¯åˆ°ç«¯æµ‹è¯• - NLP Agent")
    
    # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    print(f"{Colors.BOLD}[1/4] æ£€æŸ¥æœåŠ¡çŠ¶æ€...{Colors.RESET}")
    if not test_health_check():
        print_error("æœåŠ¡æœªè¿è¡Œï¼è¯·å…ˆå¯åŠ¨æœåŠ¡ï¼š")
        print("  uvicorn backend.app:app --reload")
        return
    print_success("æœåŠ¡æ­£å¸¸è¿è¡Œ")
    
    # è¿è¡Œå¸¸è§„æµ‹è¯•ç”¨ä¾‹
    print(f"\n{Colors.BOLD}[2/4] è¿è¡Œ {len(TEST_CASES)} ä¸ªå¸¸è§„æµ‹è¯•ç”¨ä¾‹...{Colors.RESET}")
    results = []
    passed_count = 0
    failed_count = 0
    skipped_count = 0
    
    for i, test_case in enumerate(TEST_CASES, 1):
        print(f"\n{Colors.BOLD}[{i}/{len(TEST_CASES)}]{Colors.RESET}")
        
        skip = test_case.get("skip_if_no_tavily", False)
        result = run_test_case(test_case, skip_if_no_tavily=skip)
        
        if "error" in result:
            if "Tavily" in str(result.get("error", "")):
                skipped_count += 1
                print_warning("æµ‹è¯•è¢«è·³è¿‡ï¼ˆç¼ºå°‘ Tavily APIï¼‰")
            else:
                failed_count += 1
        elif result.get("passed"):
            passed_count += 1
        else:
            failed_count += 1
        
        results.append({
            "test_case": test_case["name"],
            "test_type": "regular",
            "result": result
        })
        
        if i < len(TEST_CASES):
            time.sleep(1)
    
    # ========== æ–°å¢ï¼šè¿è¡Œå¤šæ¨¡æ€æµ‹è¯• ==========
    print(f"\n{Colors.BOLD}[3/4] è¿è¡Œ {len(MULTIMODAL_TEST_CASES)} ä¸ªå¤šæ¨¡æ€æµ‹è¯•ç”¨ä¾‹...{Colors.RESET}")
    
    import os
    if not os.path.exists(TEST_IMAGES_DIR):
        print_warning(f"æµ‹è¯•å›¾åƒç›®å½•ä¸å­˜åœ¨: {TEST_IMAGES_DIR}")
        print_info("è·³è¿‡å¤šæ¨¡æ€æµ‹è¯•ã€‚å¦‚éœ€æµ‹è¯•ï¼Œè¯·åˆ›å»ºç›®å½•å¹¶æ·»åŠ æµ‹è¯•å›¾åƒã€‚")
        multimodal_skipped = len(MULTIMODAL_TEST_CASES)
    else:
        multimodal_passed = 0
        multimodal_failed = 0
        multimodal_skipped = 0
        
        for i, test_case in enumerate(MULTIMODAL_TEST_CASES, 1):
            print(f"\n{Colors.BOLD}[å¤šæ¨¡æ€ {i}/{len(MULTIMODAL_TEST_CASES)}]{Colors.RESET}")
            
            result = run_multimodal_test_case(test_case)
            
            if "error" in result:
                if "ä¸å­˜åœ¨" in str(result.get("error", "")):
                    multimodal_skipped += 1
                    print_warning(f"è·³è¿‡æµ‹è¯•ï¼ˆå›¾åƒæ–‡ä»¶ç¼ºå¤±ï¼‰")
                else:
                    multimodal_failed += 1
            elif result.get("passed"):
                multimodal_passed += 1
            else:
                multimodal_failed += 1
            
            results.append({
                "test_case": test_case["name"],
                "test_type": "multimodal",
                "image": test_case["image_filename"],
                "result": result
            })
            
            if i < len(MULTIMODAL_TEST_CASES):
                print_info("ç­‰å¾…2ç§’...")
                time.sleep(2)
        
        # æ›´æ–°æ€»è®¡æ•°
        passed_count += multimodal_passed
        failed_count += multimodal_failed
        skipped_count += multimodal_skipped
    
    # æ‰“å°æ€»ç»“
    print_header("æµ‹è¯•æ€»ç»“")
    total_tests = len(TEST_CASES) + len(MULTIMODAL_TEST_CASES)
    print(f"{Colors.BOLD}æ€»æµ‹è¯•æ•°:{Colors.RESET} {total_tests}")
    print(f"  å¸¸è§„æµ‹è¯•: {len(TEST_CASES)} ä¸ª")
    print(f"  å¤šæ¨¡æ€æµ‹è¯•: {len(MULTIMODAL_TEST_CASES)} ä¸ª")
    print(f"\n{Colors.GREEN}âœ… é€šè¿‡: {passed_count}{Colors.RESET}")
    if failed_count > 0:
        print(f"{Colors.RED}âŒ å¤±è´¥: {failed_count}{Colors.RESET}")
    if skipped_count > 0:
        print(f"{Colors.YELLOW}â­ï¸  è·³è¿‡: {skipped_count}{Colors.RESET}")
    
    # æˆåŠŸç‡
    if total_tests > 0:
        success_rate = (passed_count / total_tests) * 100
        color = Colors.GREEN if success_rate >= 80 else Colors.YELLOW if success_rate >= 50 else Colors.RED
        print(f"\n{Colors.BOLD}æˆåŠŸç‡: {color}{success_rate:.1f}%{Colors.RESET}")
    
    # è¯¦ç»†ç»“æœï¼ˆå¤±è´¥çš„æµ‹è¯•ï¼‰
    if failed_count > 0:
        print(f"\n{Colors.BOLD}è¯¦ç»†ç»“æœ:{Colors.RESET}")
        for result in results:
            if not result["result"].get("passed") and "error" not in result["result"]:
                test_type = result.get("test_type", "regular")
                icon = "ğŸ–¼ï¸" if test_type == "multimodal" else "ğŸ“"
                print(f"\n{Colors.RED}âŒ {icon} {result['test_case']}{Colors.RESET}")
                if "error" in result["result"]:
                    print(f"  é”™è¯¯: {result['result']['error']}")
                if "errors" in result["result"]:
                    for error in result["result"]["errors"]:
                        print(f"  - {error}")
    
    # ä¿å­˜ç»“æœ
    print(f"\n{Colors.BOLD}[4/4] ä¿å­˜æµ‹è¯•ç»“æœ...{Colors.RESET}")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"test_e2e_results_{timestamp}.json"
    try:
        with open(results_file, "w", encoding="utf-8") as f:
            json.dump({
                "timestamp": timestamp,
                "summary": {
                    "total": total_tests,
                    "regular_tests": len(TEST_CASES),
                    "multimodal_tests": len(MULTIMODAL_TEST_CASES),
                    "passed": passed_count,
                    "failed": failed_count,
                    "skipped": skipped_count,
                    "success_rate": success_rate if total_tests > 0 else 0
                },
                "results": results
            }, f, ensure_ascii=False, indent=2)
        print_success(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    except Exception as e:
        print_warning(f"ä¿å­˜ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
    
    print(f"\n{Colors.BOLD}{'='*80}{Colors.RESET}")


if __name__ == "__main__":
    main()

